{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313a4a45-13c9-46c1-9ad6-b1dc90418daf",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to evaluate different RAG approaches on the tacoma manual dataset.\n",
    "\n",
    "A set of manually curated questions, answers, and relevant page numbers has been created.\n",
    "RAG answers will be compared to the curated answers for each question to evaluate each method's effectivenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d80c71-d7ea-4e60-b5ab-e141e90d3c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Documents\\tacoma_manual_rag\\rag_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CD = globals()['_dh'][0]\n",
    "sys.path.append(str(Path(CD).parent / 'rag_flask'))\n",
    "\n",
    "from data_pipeline import DataDownloadPreprocess\n",
    "import rag_query\n",
    "\n",
    "load_dotenv(dotenv_path=CD.parent / '.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3409de4d-4696-403f-9a85-c6e157024add",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_options=[\n",
    "    'Answer the QUESTION based on the CONTEXT from the user manual. Respond with the page numbers from the CONTEXT.',\n",
    "    '''Answer the QUESTION based on the CONTEXT from the user manual. \\\n",
    "If the provided CONTEXT does not provide the information needed to answer the QUESTION, then just respond with the page numbers from the CONTEXT \\\n",
    "and tell the user to look at those page numbers.''',\n",
    "    'Answer the QUESTION soley based on the CONTEXT from the user manual. Your response must include the relevant page numbers from the CONTEXT.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd0f5bf-9e35-4fdc-a094-5e789f32505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pd.read_csv('tacoma_manual_rag - question_answer.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0feddee8-4dbd-489f-bc90-d11c48285790",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c208bcc2-6288-42b2-acd1-7b8bd63530e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gemma2', 'gpt-4o-mini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3803d4ab-9e25-4521-b986-f8e4ace23e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Answer the QUESTION based on the CONTEXT from the user manual. Respond with the page numbers from the CONTEXT.',\n",
       " 'Answer the QUESTION based on the CONTEXT from the user manual. If the provided CONTEXT does not provide the information needed to answer the QUESTION, then just respond with the page numbers from the CONTEXT and tell the user to look at those page numbers.',\n",
       " 'Answer the QUESTION soley based on the CONTEXT from the user manual. Your response must include the relevant page numbers from the CONTEXT.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0d98fc7-387d-4d76-b7eb-d239aec02064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gemma2\n",
      "prompt:\n",
      " Answer the QUESTION based on the CONTEXT from the user manual. Respond with the page numbers from the CONTEXT.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 26/26 [1:35:14<00:00, 219.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      " Answer the QUESTION based on the CONTEXT from the user manual. If the provided CONTEXT does not provide the information needed to answer the QUESTION, then just respond with the page numbers from the CONTEXT and tell the user to look at those page numbers.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 26/26 [1:36:59<00:00, 223.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      " Answer the QUESTION soley based on the CONTEXT from the user manual. Your response must include the relevant page numbers from the CONTEXT.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 26/26 [1:26:19<00:00, 199.23s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print('Model:', model)\n",
    "    rq = RagQuery(\n",
    "        host='localhost',\n",
    "        llm_model=model,\n",
    "        eval_model='gemma2',\n",
    "        index_name=config['index_name'],\n",
    "        embedding_model_name=config['sentence_transformer_model_name'],\n",
    "        search_type=config['search_type'],\n",
    "        num_es_results=config['num_es_results'],\n",
    "        num_es_candidates=config['num_es_candidates'],\n",
    "        vehicle_name='Toyota Tacoma 2020',\n",
    "        similarity_threshold=config['es_knn_similarity_threshold']\n",
    "        )\n",
    "    \n",
    "    for prompt in prompt_options:\n",
    "        print('prompt:\\n', prompt, end='\\n\\n')\n",
    "        \n",
    "        for question in tqdm(q['Question'].values.tolist()):\n",
    "    \n",
    "            answer = rq.rag(query=question, \n",
    "                        evaluate=False,\n",
    "                           prompt_str=prompt)\n",
    "            model_answers.append([model, prompt, question, answer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e53967a9-c9d7-432a-aac0-f44e7da5ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answer_df = pd.DataFrame(model_answers, columns=['model', 'prompt', 'question', 'answer_dict'])\n",
    "\n",
    "for k in answer.keys():\n",
    "    model_answer_df[k] = model_answer_df['answer_dict'].map(lambda x: x.get(k, None))\n",
    "\n",
    "model_answer_df.to_csv('model_answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1dc9fce-1480-4d03-bb6b-85be0d3b5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answer_df = pd.read_csv('model_answers.csv')\n",
    "model_answer_df = model_answer_df.merge(\n",
    "    right=q.rename(columns={'Question': 'question', 'Answer': 'ground_truth_answer'}),\n",
    "    on='question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7d6a19c-30b3-4cfc-9c07-5b48cc7c3ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 18:22:22,316 - INFO - Use pytorch device_name: cpu\n",
      "2024-10-01 18:22:22,317 - INFO - Load pretrained SentenceTransformer: multi-qa-mpnet-base-dot-v1\n"
     ]
    }
   ],
   "source": [
    "# From DataTalksClub LLM Zoomcamp Module 4:\n",
    "# https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/offline-rag-evaluation.ipynb\n",
    "prompt_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "rq = rag_query.RagQuery(\n",
    "        llm_model='gpt-4o-mini',\n",
    "        eval_model='gpt-4o-mini',\n",
    "        index_name=os.getenv('INDEX_NAME'),\n",
    "        embedding_model_name=os.getenv('SENTENCE_TRANSFORMER_MODEL_NAME'),\n",
    "        search_type='knn',\n",
    "        num_neighbors=os.getenv('NUM_NEIGHBORS'),\n",
    "        num_candidates=os.getenv('NUM_CANDIDATES'),\n",
    "        host='localhost'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21003cbb-25a3-46a5-b42e-371fa151bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answer_df['eval_prompt'] = model_answer_df.apply(\n",
    "    lambda x: prompt_template.format(\n",
    "        answer_orig=x['ground_truth_answer'], question= x['question'], answer_llm=  x['answer']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a0dfc-2d3c-4862-aeb3-49ee4bc478f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "model_answer_df['eval_result'] = model_answer_df['eval_prompt'].progress_apply(lambda x: rq.llm(prompt=x, model='gpt-4o-mini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de8d3ff-5818-43fd-90da-4ea5102616cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = model_answer_df['eval_result'].values.tolist()\n",
    "eval_results = [i[0] for i in eval_results]\n",
    "eval_results = [json.loads(i.replace('”', '\"')) for i in eval_results]\n",
    "\n",
    "model_answer_df = model_answer_df.reset_index(drop=True).merge(\n",
    "    right=pd.DataFrame(eval_results),\n",
    "    left_index=True,\n",
    "    right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "25153149-5af3-4e32-b0b4-09dcac22c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model gemma2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Relevance</th>\n",
       "      <th>NON_RELEVANT</th>\n",
       "      <th>PARTLY_RELEVANT</th>\n",
       "      <th>RELEVANT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Answer the QUESTION based on the CONTEXT from the user manual. If the provided CONTEXT does not provide the information needed to answer the QUESTION, then just respond with the page numbers from the CONTEXT and tell the user to look at those page numbers.</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Answer the QUESTION based on the CONTEXT from the user manual. Respond with the page numbers from the CONTEXT.</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Answer the QUESTION soley based on the CONTEXT from the user manual. Your response must include the relevant page numbers from the CONTEXT.</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Relevance                                           NON_RELEVANT  \\\n",
       "prompt                                                             \n",
       "Answer the QUESTION based on the CONTEXT from t...            20   \n",
       "Answer the QUESTION based on the CONTEXT from t...            17   \n",
       "Answer the QUESTION soley based on the CONTEXT ...            15   \n",
       "\n",
       "Relevance                                           PARTLY_RELEVANT  RELEVANT  \n",
       "prompt                                                                         \n",
       "Answer the QUESTION based on the CONTEXT from t...                5         1  \n",
       "Answer the QUESTION based on the CONTEXT from t...                8         1  \n",
       "Answer the QUESTION soley based on the CONTEXT ...               10         1  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Results for model {models[0]}')\n",
    "\n",
    "model_answer_df.loc[model_answer_df['model'] == models[0]].groupby(\n",
    "    ['prompt', 'Relevance'], as_index=False).size().pivot(columns='Relevance', index='prompt', values='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "46556eb9-ca30-4428-aca0-f73921df8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Relevance</th>\n",
       "      <th>NON_RELEVANT</th>\n",
       "      <th>PARTLY_RELEVANT</th>\n",
       "      <th>RELEVANT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Answer the QUESTION based on the CONTEXT from the user manual. If the provided CONTEXT does not provide the information needed to answer the QUESTION, then just respond with the page numbers from the CONTEXT and tell the user to look at those page numbers.</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Answer the QUESTION based on the CONTEXT from the user manual. Respond with the page numbers from the CONTEXT.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Answer the QUESTION soley based on the CONTEXT from the user manual. Your response must include the relevant page numbers from the CONTEXT.</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Relevance                                           NON_RELEVANT  \\\n",
       "prompt                                                             \n",
       "Answer the QUESTION based on the CONTEXT from t...           2.0   \n",
       "Answer the QUESTION based on the CONTEXT from t...           NaN   \n",
       "Answer the QUESTION soley based on the CONTEXT ...           1.0   \n",
       "\n",
       "Relevance                                           PARTLY_RELEVANT  RELEVANT  \n",
       "prompt                                                                         \n",
       "Answer the QUESTION based on the CONTEXT from t...              9.0      15.0  \n",
       "Answer the QUESTION based on the CONTEXT from t...              7.0      19.0  \n",
       "Answer the QUESTION soley based on the CONTEXT ...              4.0      21.0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Results for model {models[1]}')\n",
    "\n",
    "model_answer_df.loc[model_answer_df['model'] == models[1]].groupby(\n",
    "    ['prompt', 'Relevance'], as_index=False).size().pivot(columns='Relevance', index='prompt', values='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0367a1e5-1322-4cdd-b478-a369244b7633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2</td>\n",
       "      <td>78.0</td>\n",
       "      <td>213.839553</td>\n",
       "      <td>30.574609</td>\n",
       "      <td>149.295184</td>\n",
       "      <td>192.681688</td>\n",
       "      <td>210.900636</td>\n",
       "      <td>231.422424</td>\n",
       "      <td>296.395015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.977341</td>\n",
       "      <td>1.142650</td>\n",
       "      <td>0.584887</td>\n",
       "      <td>1.232641</td>\n",
       "      <td>1.597632</td>\n",
       "      <td>2.296391</td>\n",
       "      <td>6.319951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  count        mean        std         min         25%  \\\n",
       "0       gemma2   78.0  213.839553  30.574609  149.295184  192.681688   \n",
       "1  gpt-4o-mini   78.0    1.977341   1.142650    0.584887    1.232641   \n",
       "\n",
       "          50%         75%         max  \n",
       "0  210.900636  231.422424  296.395015  \n",
       "1    1.597632    2.296391    6.319951  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics for model response times.\n",
    "model_answer_df.groupby('model', as_index=False)['response_time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8008ff8-0576-4d17-8ea8-453c2732b72e",
   "metadata": {},
   "source": [
    "Both models performed best with the third query option. It is disappointing results for the Gemma2 model, as I would prefer to run this application without an external paid service, but there is a significant difference in response time and results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
